{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu9mlt8Y0u9u"
      },
      "source": [
        "# Decision Trees and Ensemble Learning: From Scratch to Modern Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwEyMGH21Vo6"
      },
      "source": [
        "## 1. Работа с данными\n",
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvKp9G1M1cfn"
      },
      "source": [
        "### Подключение библиотек"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcrT2f1mzjDk",
        "outputId": "7138193e-15bf-4502-c040-13a8e5ef09ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: category_encoders in /usr/local/lib/python3.12/dist-packages (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (1.16.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from category_encoders) (0.14.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.5->category_encoders) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.6.0->category_encoders) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels>=0.9.0->category_encoders) (25.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.5->category_encoders) (1.17.0)\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ],
      "source": [
        "!pip install category_encoders # В google Colab не была установлена данная библиотека\n",
        "!pip install catboost # В google Colab не была установлена данная библиотека"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "iK5ODloK1l-J"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from category_encoders import CountEncoder\n",
        "import gc\n",
        "import psutil\n",
        "import scipy.special\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "import xgboost as xgb\n",
        "\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pq_ypUhm3Ykd"
      },
      "source": [
        "### Загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8XK5Pdu3cdR"
      },
      "outputs": [],
      "source": [
        "# Нужно указать релевантные пути к файлам\n",
        "train_path = \"training.csv\" \n",
        "test_path = \"test.csv\"\n",
        "\n",
        "train_df = pd.read_csv(train_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zc-JueftDTL",
        "outputId": "6b62d6a0-5c88-4141-fcb7-bd9cc78b2e81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 72983 entries, 0 to 72982\n",
            "Data columns (total 34 columns):\n",
            " #   Column                             Non-Null Count  Dtype  \n",
            "---  ------                             --------------  -----  \n",
            " 0   RefId                              72983 non-null  int64  \n",
            " 1   IsBadBuy                           72983 non-null  int64  \n",
            " 2   PurchDate                          72983 non-null  object \n",
            " 3   Auction                            72983 non-null  object \n",
            " 4   VehYear                            72983 non-null  int64  \n",
            " 5   VehicleAge                         72983 non-null  int64  \n",
            " 6   Make                               72983 non-null  object \n",
            " 7   Model                              72983 non-null  object \n",
            " 8   Trim                               70623 non-null  object \n",
            " 9   SubModel                           72975 non-null  object \n",
            " 10  Color                              72975 non-null  object \n",
            " 11  Transmission                       72974 non-null  object \n",
            " 12  WheelTypeID                        69814 non-null  float64\n",
            " 13  WheelType                          69809 non-null  object \n",
            " 14  VehOdo                             72983 non-null  int64  \n",
            " 15  Nationality                        72978 non-null  object \n",
            " 16  Size                               72978 non-null  object \n",
            " 17  TopThreeAmericanName               72978 non-null  object \n",
            " 18  MMRAcquisitionAuctionAveragePrice  72965 non-null  float64\n",
            " 19  MMRAcquisitionAuctionCleanPrice    72965 non-null  float64\n",
            " 20  MMRAcquisitionRetailAveragePrice   72965 non-null  float64\n",
            " 21  MMRAcquisitonRetailCleanPrice      72965 non-null  float64\n",
            " 22  MMRCurrentAuctionAveragePrice      72668 non-null  float64\n",
            " 23  MMRCurrentAuctionCleanPrice        72668 non-null  float64\n",
            " 24  MMRCurrentRetailAveragePrice       72668 non-null  float64\n",
            " 25  MMRCurrentRetailCleanPrice         72668 non-null  float64\n",
            " 26  PRIMEUNIT                          3419 non-null   object \n",
            " 27  AUCGUART                           3419 non-null   object \n",
            " 28  BYRNO                              72983 non-null  int64  \n",
            " 29  VNZIP1                             72983 non-null  int64  \n",
            " 30  VNST                               72983 non-null  object \n",
            " 31  VehBCost                           72983 non-null  float64\n",
            " 32  IsOnlineSale                       72983 non-null  int64  \n",
            " 33  WarrantyCost                       72983 non-null  int64  \n",
            "dtypes: float64(10), int64(9), object(15)\n",
            "memory usage: 18.9+ MB\n"
          ]
        }
      ],
      "source": [
        "train_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xrjin-HvtqhM"
      },
      "source": [
        "### Разделение данных на train, validation и test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qWhQit48t0dT"
      },
      "outputs": [],
      "source": [
        "train_df['PurchDate'] = pd.to_datetime(train_df['PurchDate'])\n",
        "train_df = train_df.sort_values('PurchDate').reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5i5_ffr-uv0y"
      },
      "outputs": [],
      "source": [
        "n = len(train_df)\n",
        "train_end = int(n * 0.33)\n",
        "valid_end = int(n * 0.66)\n",
        "\n",
        "train = train_df.iloc[:train_end]\n",
        "valid = train_df.iloc[train_end:valid_end]\n",
        "test = train_df.iloc[valid_end:]\n",
        "\n",
        "y_train = train['IsBadBuy']\n",
        "y_valid = valid['IsBadBuy']\n",
        "y_test = test['IsBadBuy']\n",
        "\n",
        "X_train = train.drop(['IsBadBuy', 'PurchDate'], axis = 1)\n",
        "X_valid = valid.drop(['IsBadBuy', 'PurchDate'], axis = 1)\n",
        "X_test = test.drop(['IsBadBuy', 'PurchDate'], axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eEwf1dXzyg-"
      },
      "source": [
        "**OneHotEncoder**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nE6fXK00Afp"
      },
      "source": [
        "Преобразует категориальные признаки в бинарные столбцы. Параметр handle_unknown='ignore' присваивает нули новым категориям в валидационной/тестовой выборках, предотвращая ошибки. Это увеличивает размерность данных (особенно если много категорий), но подходит для моделей, устойчивых к высокой размерности (например, деревья)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GLhXLhX1KPi"
      },
      "source": [
        "- **Плюсы:** Простота интерпретации, подходит для большинства моделей, обрабатывает новые категории через handle_unknown='ignore'.\n",
        "- **Минусы:** Увеличивает размерность данных (например, столбец \"Make\" с 50 уникальными значениями создаёт 50 новых столбцов), что может замедлить обучение на больших датасетах.\n",
        "- **Когда использовать:** Если датасет небольшой или модель (например, CatBoost) не имеет встроенной обработки категорий."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTgbjdBAyTSs",
        "outputId": "309a94f0-1e42-477f-b818-397defefa6e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Размер train: (24084, 1713), valid: (24084, 1713), test: (24815, 1713)\n"
          ]
        }
      ],
      "source": [
        "cat_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)\n",
        "    ], remainder='passthrough'\n",
        ")\n",
        "\n",
        "X_train_enc = preprocessor.fit_transform(X_train)\n",
        "X_valid_enc = preprocessor.transform(X_valid)\n",
        "X_test_enc = preprocessor.transform(X_test)\n",
        "\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "X_train_enc = pd.DataFrame(X_train_enc, columns=feature_names)\n",
        "X_valid_enc = pd.DataFrame(X_valid_enc, columns=feature_names)\n",
        "X_test_enc = pd.DataFrame(X_test_enc, columns=feature_names)\n",
        "\n",
        "print(f'Размер train: {X_train_enc.shape}, valid: {X_valid_enc.shape}, test: {X_test_enc.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2wH7CJVz4uz"
      },
      "source": [
        "**CountEncoder**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG-QKFjs0FId"
      },
      "source": [
        "Альтернатива, заменяющая категории их частотой в тренировочной выборке. Новые категории получают значение 1 (handle_unknown=1). Это сохраняет меньшую размерность, что полезно для больших датасетов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biri8pyA1S0T"
      },
      "source": [
        "- **Плюсы:** Сохраняет меньшую размерность, так как каждая категория заменяется одним числом (частотой). Хорошо работает с деревьями, которые нечувствительны к масштабу признаков.\n",
        "- **Минусы:** Потеря информации о категориях, менее интерпретируемо.\n",
        "- **Когда использовать:** Для больших датасетов или если много новых категорий в valid/test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hbog4r-kzb-v",
        "outputId": "617820e3-57a9-4262-c7aa-8c8091fdd3e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Альтернативный подход (CountEncoder) - Размер train: (24084, 32), valid: (24084, 32), test: (24815, 32)\n"
          ]
        }
      ],
      "source": [
        "preprocessor_alt = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', CountEncoder(handle_unknown=1), cat_cols)\n",
        "    ], remainder='passthrough'\n",
        ")\n",
        "\n",
        "X_train_enc_alt = preprocessor_alt.fit_transform(X_train)\n",
        "X_valid_enc_alt = preprocessor_alt.transform(X_valid)\n",
        "X_test_enc_alt = preprocessor_alt.transform(X_test)\n",
        "\n",
        "feature_names_alt = preprocessor_alt.get_feature_names_out()\n",
        "X_train_enc_alt = pd.DataFrame(X_train_enc_alt, columns=feature_names_alt)\n",
        "X_valid_enc_alt = pd.DataFrame(X_valid_enc_alt, columns=feature_names_alt)\n",
        "X_test_enc_alt = pd.DataFrame(X_test_enc_alt, columns=feature_names_alt)\n",
        "\n",
        "print(f'Альтернативный подход (CountEncoder) - Размер train: {X_train_enc_alt.shape}, valid: {X_valid_enc_alt.shape}, test: {X_test_enc_alt.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve1ci0jH2QDD"
      },
      "source": [
        "### Релизация собственной функции для расчета Gini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SH_76dUG2Wk-"
      },
      "outputs": [],
      "source": [
        "def gini_score(y_true, y_pred_proba):\n",
        "    if y_pred_proba.ndim == 1:\n",
        "        auc = roc_auc_score(y_true, y_pred_proba)\n",
        "    else:\n",
        "        auc = roc_auc_score(y_true, y_pred_proba[:, 1])\n",
        "    return 2 * auc - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZCzyAwD2muj"
      },
      "source": [
        "## 2. Реализация собственного дерева решений (CART)\n",
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhCFiVLn3Ng0"
      },
      "source": [
        "В этом блоке реализуем собственные классы DecisionTreeClassifier и DecisionTreeRegressor. Класс Node хранит данные и вычисляет нечистоту (impurity). Для классификации используем критерий Джини, для регрессии — стандартное отклонение. Также добавляем функцию для случайных разделений (для ExtraTrees)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6exTi3r6IJW"
      },
      "source": [
        "**Класс Node**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ttL129NM2-Aj"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "    def __init__(self, indices, depth=0):\n",
        "        self.indices = indices\n",
        "        self.depth = depth\n",
        "        self.feature = None\n",
        "        self.threshold = None\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "        self.value = None\n",
        "\n",
        "    def gini(self, y):\n",
        "        if len(self.indices) == 0:\n",
        "            return 0\n",
        "        p = np.bincount(y[self.indices].astype(int)) / len(self.indices)\n",
        "        return 1 - np.sum(p**2)\n",
        "\n",
        "    def std(self, y):\n",
        "        return np.std(y[self.indices]) if len(self.indices) > 0 else 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A53hXqw-6Kv2"
      },
      "source": [
        "**Класс DecisionTreeBase**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HBvWsXJw6Qc8"
      },
      "outputs": [],
      "source": [
        "class DecisionTreeBase:\n",
        "    def __init__(self, max_depth=None, is_classifier=True, min_samples_split=2):\n",
        "        self.max_depth = max_depth\n",
        "        self.is_classifier = is_classifier\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.root = None\n",
        "        self.X = None\n",
        "        self.y = None\n",
        "\n",
        "    def _impurity(self, node, y):\n",
        "        return node.gini(y) if self.is_classifier else node.std(y)\n",
        "\n",
        "    def _find_best_split(self, node):\n",
        "        best_gain = -np.inf\n",
        "        best_feature = None\n",
        "        best_threshold = None\n",
        "\n",
        "        for feature in range(self.X.shape[1]):\n",
        "            values = np.unique(self.X[node.indices, feature])\n",
        "            for threshold in values:\n",
        "                left_idx = node.indices[self.X[node.indices, feature] <= threshold]\n",
        "                right_idx = node.indices[self.X[node.indices, feature] > threshold]\n",
        "\n",
        "                if len(left_idx) < self.min_samples_split or len(right_idx) < self.min_samples_split:\n",
        "                    continue\n",
        "\n",
        "                impurity_parent = self._impurity(node, self.y)\n",
        "                impurity_left = (1 - np.sum((np.bincount(self.y[left_idx].astype(int))/len(left_idx))**2) if len(left_idx)>0 else 0) if self.is_classifier else (self.y[left_idx].std() if len(left_idx)>0 else 0)\n",
        "                impurity_right = (1 - np.sum((np.bincount(self.y[right_idx].astype(int))/len(right_idx))**2) if len(right_idx)>0 else 0) if self.is_classifier else (self.y[right_idx].std() if len(right_idx)>0 else 0)\n",
        "\n",
        "                gain = impurity_parent - (\n",
        "                    len(left_idx)/len(node.indices) * impurity_left +\n",
        "                    len(right_idx)/len(node.indices) * impurity_right\n",
        "                )\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feature = feature\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        return best_feature, best_threshold, best_gain\n",
        "\n",
        "    def _build_tree(self, node):\n",
        "        #print(f\"Глубина: {node.depth}, Размер узла: {len(node.indices)}\")\n",
        "        if (node.depth == self.max_depth or\n",
        "            len(np.unique(self.y[node.indices])) == 1 or\n",
        "            len(node.indices) < self.min_samples_split):\n",
        "            if self.is_classifier:\n",
        "                node.value = np.bincount(self.y[node.indices].astype(int), minlength=2) / len(node.indices)\n",
        "            else:\n",
        "                node.value = np.mean(self.y[node.indices])\n",
        "            return\n",
        "\n",
        "        feature, threshold, gain = self._find_best_split(node)\n",
        "        if feature is None or gain <= 0:\n",
        "            if self.is_classifier:\n",
        "                node.value = np.bincount(self.y[node.indices].astype(int), minlength=2) / len(node.indices)\n",
        "            else:\n",
        "                node.value = np.mean(self.y[node.indices])\n",
        "            return\n",
        "\n",
        "        node.feature = feature\n",
        "        node.threshold = threshold\n",
        "\n",
        "        left_idx = node.indices[self.X[node.indices, feature] <= threshold]\n",
        "        right_idx = node.indices[self.X[node.indices, feature] > threshold]\n",
        "\n",
        "        node.left = Node(left_idx, node.depth + 1)\n",
        "        node.right = Node(right_idx, node.depth + 1)\n",
        "\n",
        "        self._build_tree(node.left)\n",
        "        self._build_tree(node.right)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.X = np.array(X)\n",
        "        self.y = np.array(y)\n",
        "        self.root = Node(np.arange(len(self.y)))\n",
        "        self._build_tree(self.root)\n",
        "        self.X = None  # Очистка\n",
        "        self.y = None\n",
        "        gc.collect()\n",
        "\n",
        "    def _predict_one(self, x, node):\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._predict_one(x, node.left)\n",
        "        else:\n",
        "            return self._predict_one(x, node.right)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if not self.is_classifier:\n",
        "            raise ValueError(\"predict_proba только для классификатора\")\n",
        "        return np.array([self._predict_one(row, self.root) for row in np.array(X)])\n",
        "\n",
        "    def predict(self, X):\n",
        "        preds = np.array([self._predict_one(row, self.root) for row in np.array(X)])\n",
        "        if self.is_classifier:\n",
        "            return np.argmax(preds, axis=1) if len(preds.shape) > 1 else preds\n",
        "        else:\n",
        "            return preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyVmMRZLBz0A"
      },
      "source": [
        "**Класс DecisionTreeClassifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AnDYOP0u7mo6"
      },
      "outputs": [],
      "source": [
        "class DecisionTreeClassifier(DecisionTreeBase):\n",
        "    def __init__(self, max_depth=None, min_samples_split=2):\n",
        "        super().__init__(max_depth, is_classifier=True, min_samples_split=min_samples_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Q1G9vTWB51t"
      },
      "source": [
        "**Класс DecisionTreeRegressor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BKxkwE79B9jY"
      },
      "outputs": [],
      "source": [
        "class DecisionTreeRegressor(DecisionTreeBase):\n",
        "    def __init__(self, max_depth=None, min_samples_split=2):\n",
        "        super().__init__(max_depth, is_classifier=False, min_samples_split=min_samples_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ag4Es4YCCE2"
      },
      "source": [
        "**Функция для случайного разделения (для ExtraTrees)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "IrEupJ1oCEoP"
      },
      "outputs": [],
      "source": [
        "def find_random_split(node, X, y, is_classifier):\n",
        "    best_gain = -np.inf\n",
        "    best_feature = None\n",
        "    best_threshold = None\n",
        "\n",
        "    for feature in range(X.shape[1]):\n",
        "        values = np.unique(X[node.indices, feature])\n",
        "        if len(values) < 2:\n",
        "            continue\n",
        "        threshold = np.random.choice(values)\n",
        "        left_idx = node.indices[X[node.indices, feature] <= threshold]\n",
        "        right_idx = node.indices[X[node.indices, feature] > threshold]\n",
        "\n",
        "        if len(left_idx) < 2 or len(right_idx) < 2:\n",
        "            continue\n",
        "\n",
        "        impurity_parent = node.gini(y) if is_classifier else node.std(y)\n",
        "        impurity_left = (1 - np.sum((np.bincount(y[left_idx].astype(int))/len(left_idx))**2) if len(left_idx)>0 else 0) if is_classifier else (y[left_idx].std() if len(left_idx)>0 else 0)\n",
        "        impurity_right = (1 - np.sum((np.bincount(y[right_idx].astype(int))/len(right_idx))**2) if len(right_idx)>0 else 0) if is_classifier else (y[right_idx].std() if len(right_idx)>0 else 0)\n",
        "\n",
        "        gain = impurity_parent - (len(left_idx)/len(node.indices) * impurity_left + len(right_idx)/len(node.indices) * impurity_right)\n",
        "\n",
        "        if gain > best_gain:\n",
        "            best_gain = gain\n",
        "            best_feature = feature\n",
        "            best_threshold = threshold\n",
        "\n",
        "    return best_feature, best_threshold, best_gain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG2pL6nMCJKw"
      },
      "source": [
        "## 3. Оценка собственного дерева на валидационной выборке\n",
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb5UF2cSEbQz"
      },
      "source": [
        "Обучаем DecisionTreeClassifier на тренировочных данных и вычисляем коэффициент Джини на валидационной выборке. При необходимости настраиваем max_depth для достижения Джини >= 0.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_A80DQH2EdZY",
        "outputId": "13fd50cd-2e6a-42ff-9dc4-b9cd38b0c279"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Классы в y_train: [0 1]\n",
            "Классы в y_valid: [0 1]\n",
            "Пример X_train_enc:    cat__Auction_ADESA  cat__Auction_MANHEIM  cat__Auction_OTHER  \\\n",
            "0                 0.0                   1.0                 0.0   \n",
            "1                 0.0                   1.0                 0.0   \n",
            "2                 0.0                   1.0                 0.0   \n",
            "3                 0.0                   1.0                 0.0   \n",
            "4                 0.0                   1.0                 0.0   \n",
            "\n",
            "   cat__Make_ACURA  cat__Make_BUICK  cat__Make_CADILLAC  cat__Make_CHEVROLET  \\\n",
            "0              0.0              0.0                 0.0                  0.0   \n",
            "1              0.0              0.0                 0.0                  0.0   \n",
            "2              0.0              0.0                 0.0                  0.0   \n",
            "3              0.0              0.0                 0.0                  1.0   \n",
            "4              0.0              0.0                 0.0                  0.0   \n",
            "\n",
            "   cat__Make_CHRYSLER  cat__Make_DODGE  cat__Make_FORD  ...  \\\n",
            "0                 1.0              0.0             0.0  ...   \n",
            "1                 0.0              0.0             1.0  ...   \n",
            "2                 0.0              1.0             0.0  ...   \n",
            "3                 0.0              0.0             0.0  ...   \n",
            "4                 0.0              0.0             1.0  ...   \n",
            "\n",
            "   remainder__MMRAcquisitonRetailCleanPrice  \\\n",
            "0                                   10066.0   \n",
            "1                                    6693.0   \n",
            "2                                    4886.0   \n",
            "3                                   11174.0   \n",
            "4                                    5068.0   \n",
            "\n",
            "   remainder__MMRCurrentAuctionAveragePrice  \\\n",
            "0                                    8709.0   \n",
            "1                                    4908.0   \n",
            "2                                    3397.0   \n",
            "3                                    9202.0   \n",
            "4                                    3369.0   \n",
            "\n",
            "   remainder__MMRCurrentAuctionCleanPrice  \\\n",
            "0                                 10331.0   \n",
            "1                                  5971.0   \n",
            "2                                  4272.0   \n",
            "3                                 10794.0   \n",
            "4                                  4492.0   \n",
            "\n",
            "   remainder__MMRCurrentRetailAveragePrice  \\\n",
            "0                                   9906.0   \n",
            "1                                   5801.0   \n",
            "2                                   4169.0   \n",
            "3                                  10438.0   \n",
            "4                                   4139.0   \n",
            "\n",
            "   remainder__MMRCurrentRetailCleanPrice  remainder__BYRNO  remainder__VNZIP1  \\\n",
            "0                                11657.0            3453.0            80022.0   \n",
            "1                                 6949.0           22916.0            80022.0   \n",
            "2                                 5114.0            3453.0            80022.0   \n",
            "3                                12158.0           22916.0            80022.0   \n",
            "4                                 5351.0           22916.0            80022.0   \n",
            "\n",
            "   remainder__VehBCost  remainder__IsOnlineSale  remainder__WarrantyCost  \n",
            "0               6770.0                      0.0                   1389.0  \n",
            "1               6160.0                      0.0                    941.0  \n",
            "2               4250.0                      0.0                   1155.0  \n",
            "3               8180.0                      0.0                   1703.0  \n",
            "4               4900.0                      0.0                    825.0  \n",
            "\n",
            "[5 rows x 1713 columns]\n",
            "Доля ненулевых значений в X_train_enc: 0.0180346845972934\n",
            "Пример вероятностей: [[0.95136314 0.04863686]\n",
            " [0.95536562 0.04463438]\n",
            " [0.90987124 0.09012876]\n",
            " [0.33333333 0.66666667]\n",
            " [0.92857143 0.07142857]]\n",
            "Джини собственного DT на валидации: 0.20203713111439425\n"
          ]
        }
      ],
      "source": [
        "# Проверка данных\n",
        "print(\"Классы в y_train:\", np.unique(y_train))\n",
        "print(\"Классы в y_valid:\", np.unique(y_valid))\n",
        "print(\"Пример X_train_enc:\", X_train_enc.head())\n",
        "print(\"Доля ненулевых значений в X_train_enc:\", np.mean(X_train_enc.values != 0))\n",
        "\n",
        "# Обучение\n",
        "model = DecisionTreeClassifier(max_depth=7, min_samples_split=2)\n",
        "model.fit(X_train_enc.values, y_train.values)\n",
        "\n",
        "# Предсказание\n",
        "proba_valid = model.predict_proba(X_valid_enc.values)\n",
        "print(\"Пример вероятностей:\", proba_valid[:5])  # Диагностика\n",
        "gini_custom = gini_score(y_valid, proba_valid)\n",
        "print(f'Джини собственного DT на валидации: {gini_custom}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNMOWRrhHf-0"
      },
      "source": [
        "## 4. Сравнение с DecisionTreeClassifier из sklearn\n",
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEiK-larHrWv"
      },
      "source": [
        "Используем DecisionTreeClassifier из sklearn для сравнения. Проверяем, лучше ли он собственной реализации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI8K0CHnHtEQ",
        "outputId": "40181dd7-cbaa-4a9a-c72a-1f81c20092d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Джини sklearn DT на валидации: 0.43332538187431724\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier as SkDT\n",
        "\n",
        "sk_model = SkDT(max_depth=7, criterion='gini', random_state=42)\n",
        "sk_model.fit(X_train_enc, y_train)\n",
        "\n",
        "sk_proba = sk_model.predict_proba(X_valid_enc)\n",
        "sk_gini = gini_score(y_valid, sk_proba)\n",
        "print(f'Джини sklearn DT на валидации: {sk_gini}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0L3qM59H0vp"
      },
      "source": [
        "Резюме результатов: Sklearn обычно лучше благодаря оптимизированному поиску разделений и реализации на C++. Собственная версия медленнее и менее эффективна."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf4CyG8iH6OG"
      },
      "source": [
        "## 5. Реализация собственного RandomForestClassifier\n",
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhZnXZ4TJEwB"
      },
      "source": [
        "Реализуем Random Forest с использованием бэггинга и случайного выбора признаков. Устанавливаем фиксированный random seed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xBIgZXEaJIRw"
      },
      "outputs": [],
      "source": [
        "class RandomForestClassifier:\n",
        "    def __init__(self, n_trees=20, max_depth=5, max_features=10, random_seed=42):\n",
        "        self.n_trees = n_trees\n",
        "        self.max_depth = max_depth\n",
        "        self.max_features = max_features\n",
        "        self.random_seed = random_seed\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        np.random.seed(self.random_seed)\n",
        "        n_samples, n_feats = X.shape\n",
        "        mf = min(self.max_features, n_feats)  # Ограничение признаков\n",
        "\n",
        "        for i in range(self.n_trees):\n",
        "            print(f\"Обучение дерева {i+1}/{self.n_trees}\")\n",
        "            idx = np.random.choice(n_samples, n_samples, replace=True)\n",
        "            X_boot = X[idx]\n",
        "            y_boot = y[idx]\n",
        "\n",
        "            feat_idx = np.random.choice(n_feats, mf, replace=False)\n",
        "            X_boot_sub = X_boot[:, feat_idx]\n",
        "\n",
        "            tree = DecisionTreeClassifier(max_depth=self.max_depth, min_samples_split=2)\n",
        "            tree.fit(X_boot_sub, y_boot)\n",
        "            self.trees.append((tree, feat_idx))\n",
        "            gc.collect()  # Очистка памяти\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        probs = []\n",
        "        for tree, feat_idx in self.trees:\n",
        "            probs.append(tree.predict_proba(X[:, feat_idx]))\n",
        "        return np.mean(probs, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmbSoXWHXesQ"
      },
      "source": [
        "Обучение и оценка"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMkk9K-FQORO",
        "outputId": "2dffdb14-ce16-42bb-bddb-352dc8753ed6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage before RF: 21.7%\n",
            "Обучение дерева 1/30\n",
            "Обучение дерева 2/30\n",
            "Обучение дерева 3/30\n",
            "Обучение дерева 4/30\n",
            "Обучение дерева 5/30\n",
            "Обучение дерева 6/30\n",
            "Обучение дерева 7/30\n",
            "Обучение дерева 8/30\n",
            "Обучение дерева 9/30\n",
            "Обучение дерева 10/30\n",
            "Обучение дерева 11/30\n",
            "Обучение дерева 12/30\n",
            "Обучение дерева 13/30\n",
            "Обучение дерева 14/30\n",
            "Обучение дерева 15/30\n",
            "Обучение дерева 16/30\n",
            "Обучение дерева 17/30\n",
            "Обучение дерева 18/30\n",
            "Обучение дерева 19/30\n",
            "Обучение дерева 20/30\n",
            "Обучение дерева 21/30\n",
            "Обучение дерева 22/30\n",
            "Обучение дерева 23/30\n",
            "Обучение дерева 24/30\n",
            "Обучение дерева 25/30\n",
            "Обучение дерева 26/30\n",
            "Обучение дерева 27/30\n",
            "Обучение дерева 28/30\n",
            "Обучение дерева 29/30\n",
            "Обучение дерева 30/30\n",
            "RAM usage after RF: 22.1%\n",
            "Джини собственного RF на валидации: 0.37846974201500316\n"
          ]
        }
      ],
      "source": [
        "# Проверка использования памяти\n",
        "print(f\"RAM usage before RF: {psutil.virtual_memory().percent}%\")\n",
        "\n",
        "# Обучение и оценка\n",
        "rf = RandomForestClassifier(n_trees=30, max_depth=7, max_features=10, random_seed=42)\n",
        "rf.fit(X_train_enc.values, y_train.values)\n",
        "rf_proba = rf.predict_proba(X_valid_enc.values)\n",
        "rf_gini = gini_score(y_valid, rf_proba)\n",
        "print(f\"RAM usage after RF: {psutil.virtual_memory().percent}%\")\n",
        "print(f'Джини собственного RF на валидации: {rf_gini}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dJpJlDtWcJy"
      },
      "source": [
        "Резюме результатов: Улучшает результат одиночного дерева:\n",
        "\n",
        "- На одиночном - Gini: 0.20203713111439425\n",
        "\n",
        "- На RandomForest - Gini: 0.37846974201500316"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGFo6YcJXDrK"
      },
      "source": [
        "## 6. Реализация собственного GBDT Classifier\n",
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_k_n0U_XL4X"
      },
      "source": [
        "Реализуем градиентный бустинг, используя регрессионные деревья для градиентов (BCE loss). Используем learning rate и выборку признаков."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NT_63abHXQcq"
      },
      "outputs": [],
      "source": [
        "class GBDTClassifier:\n",
        "    def __init__(self, n_trees=100, max_depth=3, max_features='sqrt', lr=0.1, random_seed=42):\n",
        "        self.n_trees = n_trees\n",
        "        self.max_depth = max_depth\n",
        "        self.max_features = max_features\n",
        "        self.lr = lr\n",
        "        self.random_seed = random_seed\n",
        "        self.trees = []\n",
        "        self.init_pred = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        np.random.seed(self.random_seed)\n",
        "        y = y.astype(float)\n",
        "        self.init_pred = np.log(np.mean(y) / (1 - np.mean(y) + 1e-10))\n",
        "        F = np.full(len(y), self.init_pred)\n",
        "\n",
        "        for _ in range(self.n_trees):\n",
        "            p = scipy.special.expit(F)\n",
        "            residuals = y - p\n",
        "\n",
        "            n_feats = X.shape[1]\n",
        "            mf = int(np.sqrt(n_feats)) if self.max_features == 'sqrt' else self.max_features\n",
        "            feat_idx = np.random.choice(n_feats, mf, replace=False)\n",
        "            X_sub = X[:, feat_idx]\n",
        "\n",
        "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
        "            tree.fit(X_sub, residuals)\n",
        "            self.trees.append((tree, feat_idx))\n",
        "\n",
        "            tree_pred = tree.predict(X_sub)\n",
        "            F += self.lr * tree_pred\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        F = np.full(X.shape[0], self.init_pred)\n",
        "        for tree, feat_idx in self.trees:\n",
        "            F += self.lr * tree.predict(X[:, feat_idx])\n",
        "        p = scipy.special.expit(F)\n",
        "        return np.vstack((1 - p, p)).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvy26I5eXhjO"
      },
      "source": [
        "Обучение и оценка"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APtKTH0_XXn3",
        "outputId": "f7dd7fb7-70ae-405d-ae77-b6d0a07752fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Джини собственного GBDT на валидации: 0.4502569959573457\n"
          ]
        }
      ],
      "source": [
        "# Обучение и оценка\n",
        "gbdt = GBDTClassifier(n_trees=100, max_depth=3, lr=0.1)\n",
        "gbdt.fit(X_train_enc.values, y_train.values)\n",
        "gbdt_proba = gbdt.predict_proba(X_valid_enc.values)\n",
        "gbdt_gini = gini_score(y_valid, gbdt_proba)\n",
        "print(f'Джини собственного GBDT на валидации: {gbdt_gini}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Loeu67WMHcPn"
      },
      "source": [
        "## 7. Современные реализации GBDT (LightGBM, CatBoost, XGBoost)\n",
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnNF2DCJHqAY"
      },
      "source": [
        "Обучаем и сравниваем LightGBM, CatBoost и XGBoost с настройкой параметров. Отмечаем различия в обработке данных (например, CatBoost для категориальных признаков)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "809TkfruHsZL"
      },
      "outputs": [],
      "source": [
        "# Функция для обучения и оценки\n",
        "def train_and_eval_lib(model_type, X_train, y_train, X_valid, y_valid):\n",
        "    if model_type == 'lgb':\n",
        "        dtrain = lgb.Dataset(X_train, y_train)\n",
        "        dvalid = lgb.Dataset(X_valid, y_valid)\n",
        "        params = {'objective': 'binary', 'metric': 'auc', 'learning_rate': 0.1, 'max_depth': 6, 'seed': 42}\n",
        "        bst = lgb.train(params, dtrain, num_boost_round=1000, valid_sets=[dvalid], callbacks=[lgb.early_stopping(50)])\n",
        "        proba = bst.predict(X_valid)\n",
        "    elif model_type == 'cb':\n",
        "        model = cb.CatBoostClassifier(iterations=1000, depth=6, learning_rate=0.1, eval_metric='AUC', random_seed=42)\n",
        "        model.fit(X_train, y_train, eval_set=(X_valid, y_valid), early_stopping_rounds=50, verbose=0)\n",
        "        proba = model.predict_proba(X_valid)[:, 1]\n",
        "    elif model_type == 'xgb':\n",
        "        dtrain = xgb.DMatrix(X_train, y_train)\n",
        "        dvalid = xgb.DMatrix(X_valid, y_valid)\n",
        "        params = {'objective': 'binary:logistic', 'eval_metric': 'auc', 'learning_rate': 0.1, 'max_depth': 6, 'seed': 42}\n",
        "        bst = xgb.train(params, dtrain, num_boost_round=1000, evals=[(dvalid, 'valid')], early_stopping_rounds=50, verbose_eval=False)\n",
        "        proba = bst.predict(dvalid)\n",
        "\n",
        "    gini = gini_score(y_valid, proba)\n",
        "    return gini"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2XVKMdtIMST"
      },
      "source": [
        "Запуск и оценка"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EgSAq7IIOUC",
        "outputId": "c5adff8b-4fe5-4e98-be55-7d96077d7bcb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Provided parameters constrain tree depth (max_depth=6) without explicitly setting 'num_leaves'. This can lead to underfitting. To resolve this warning, pass 'num_leaves' (<=64) in params. Alternatively, pass (max_depth=-1) and just use 'num_leaves' to constrain model complexity.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Warning] Provided parameters constrain tree depth (max_depth=6) without explicitly setting 'num_leaves'. This can lead to underfitting. To resolve this warning, pass 'num_leaves' (<=64) in params. Alternatively, pass (max_depth=-1) and just use 'num_leaves' to constrain model complexity.\n",
            "[LightGBM] [Info] Number of positive: 2756, number of negative: 21328\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003209 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 4222\n",
            "[LightGBM] [Info] Number of data points in the train set: 24084, number of used features: 535\n",
            "[LightGBM] [Warning] Provided parameters constrain tree depth (max_depth=6) without explicitly setting 'num_leaves'. This can lead to underfitting. To resolve this warning, pass 'num_leaves' (<=64) in params. Alternatively, pass (max_depth=-1) and just use 'num_leaves' to constrain model complexity.\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.114433 -> initscore=-2.046240\n",
            "[LightGBM] [Info] Start training from score -2.046240\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Early stopping, best iteration is:\n",
            "[47]\tvalid_0's auc: 0.741204\n",
            "LightGBM Джини: 0.4824079719500127\n",
            "CatBoost Джини: 0.4917406867603127\n",
            "XGBoost Джини: 0.49174692122069263\n"
          ]
        }
      ],
      "source": [
        "lgb_gini = train_and_eval_lib('lgb', X_train_enc, y_train, X_valid_enc, y_valid)\n",
        "cb_gini = train_and_eval_lib('cb', X_train_enc, y_train, X_valid_enc, y_valid)\n",
        "xgb_gini = train_and_eval_lib('xgb', X_train_enc, y_train, X_valid_enc, y_valid)\n",
        "\n",
        "print(f'LightGBM Джини: {lgb_gini}')\n",
        "print(f'CatBoost Джини: {cb_gini}')\n",
        "print(f'XGBoost Джини: {xgb_gini}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlFML1VwJyYJ"
      },
      "source": [
        "Лучшая модель по итогу - XGBoost с показателем **Gini:**\n",
        "\n",
        "**0.49174692122069263**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZOa6eWmJslw"
      },
      "source": [
        "## 8. Оценка лучшей модели на тестовой выборке\n",
        "------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4027GA7LJ_eE"
      },
      "source": [
        "Выбираем XGBoost, обучаем заново на тренировочной выборке и вычисляем Джини на train/valid/test. Проверяем на переобучение."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cv6RVQ0xKJHS",
        "outputId": "90e6cb03-0296-4e2f-9e7d-0cdef1f45789"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Классы в y_train: (array([0, 1]), array([21328,  2756]))\n",
            "Классы в y_valid: (array([0, 1]), array([20926,  3158]))\n",
            "Классы в y_test: (array([0, 1]), array([21753,  3062]))\n",
            "Размер X_train_enc: (24084, 1713)\n",
            "RAM usage before training: 27.7%\n",
            "Пример вероятностей на train (первые 5): [0.04979338 0.06662174 0.12935837 0.05134354 0.10736223]\n",
            "Пример вероятностей на valid (первые 5): [0.02458949 0.04551674 0.0472426  0.09970537 0.03983329]\n",
            "Пример вероятностей на test (первые 5): [0.10006604 0.08838389 0.09167034 0.03559981 0.04693903]\n",
            "Уникальные вероятности на valid: [0.0065335  0.00676447 0.00701582 ... 0.9775835  0.9779408  0.98158205]\n",
            "XGBoost (лучшая модель) — Джини на train: 0.7128217898995794\n",
            "Джини на valid: 0.49174692122069263\n",
            "Джини на test: 0.5065977821238228\n",
            "RAM usage after training: 37.5%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "85"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Классы в y_train:\", np.unique(y_train, return_counts=True))\n",
        "print(\"Классы в y_valid:\", np.unique(y_valid, return_counts=True))\n",
        "print(\"Классы в y_test:\", np.unique(y_test, return_counts=True))\n",
        "print(f\"Размер X_train_enc: {X_train_enc.shape}\")\n",
        "print(f\"RAM usage before training: {psutil.virtual_memory().percent}%\")\n",
        "\n",
        "# Приведение типов\n",
        "y_train = y_train.astype(int)\n",
        "y_valid = y_valid.astype(int)\n",
        "y_test = y_test.astype(int)\n",
        "X_train_enc = X_train_enc.astype(float)\n",
        "X_valid_enc = X_valid_enc.astype(float)\n",
        "X_test_enc = X_test_enc.astype(float)\n",
        "\n",
        "# Обучение XGBoost\n",
        "dtrain = xgb.DMatrix(X_train_enc, y_train)\n",
        "dvalid = xgb.DMatrix(X_valid_enc, y_valid)\n",
        "dtest = xgb.DMatrix(X_test_enc, y_test)\n",
        "params = {\n",
        "    'objective': 'binary:logistic',\n",
        "    'eval_metric': 'auc',\n",
        "    'learning_rate': 0.1,\n",
        "    'max_depth': 6,\n",
        "    'seed': 42\n",
        "}\n",
        "best_model = xgb.train(\n",
        "    params,\n",
        "    dtrain,\n",
        "    num_boost_round=1000,\n",
        "    evals=[(dvalid, 'valid')],\n",
        "    early_stopping_rounds=50,\n",
        "    verbose_eval=False\n",
        ")\n",
        "\n",
        "# Предсказания\n",
        "train_proba = best_model.predict(dtrain)\n",
        "valid_proba = best_model.predict(dvalid)\n",
        "test_proba = best_model.predict(dtest)\n",
        "\n",
        "# Проверка вероятностей\n",
        "print(\"Пример вероятностей на train (первые 5):\", train_proba[:5])\n",
        "print(\"Пример вероятностей на valid (первые 5):\", valid_proba[:5])\n",
        "print(\"Пример вероятностей на test (первые 5):\", test_proba[:5])\n",
        "print(\"Уникальные вероятности на valid:\", np.unique(valid_proba))\n",
        "\n",
        "# Вычисление Джини\n",
        "train_gini = gini_score(y_train, train_proba)\n",
        "valid_gini = gini_score(y_valid, valid_proba)\n",
        "test_gini = gini_score(y_test, test_proba)\n",
        "\n",
        "print(f\"XGBoost (лучшая модель) — Джини на train: {train_gini}\")\n",
        "print(f\"Джини на valid: {valid_gini}\")\n",
        "print(f\"Джини на test: {test_gini}\")\n",
        "print(f\"RAM usage after training: {psutil.virtual_memory().percent}%\")\n",
        "\n",
        "# Очистка памяти\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJQIJQrIKQTT"
      },
      "source": [
        "## 9. Реализация ExtraTreesClassifier\n",
        "-------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV5CqNiqKahv"
      },
      "source": [
        "Реализуем ExtraTrees: без бутстрапа по строкам, с случайными разделениями."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Wrgmsp4qKWhJ"
      },
      "outputs": [],
      "source": [
        "class ExtraTreesClassifier:\n",
        "    def __init__(self, n_trees=20, max_depth=5, max_features=10, random_seed=42):\n",
        "        self.n_trees = n_trees\n",
        "        self.max_depth = max_depth\n",
        "        self.max_features = max_features\n",
        "        self.random_seed = random_seed\n",
        "        self.trees = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        np.random.seed(self.random_seed)\n",
        "        n_samples, n_feats = X.shape\n",
        "        mf = min(self.max_features, n_feats)  # Ограничение признаков\n",
        "\n",
        "        for i in range(self.n_trees):\n",
        "            #print(f\"Обучение дерева {i+1}/{self.n_trees}\")\n",
        "            feat_idx = np.random.choice(n_feats, mf, replace=False)\n",
        "            X_sub = X[:, feat_idx]\n",
        "\n",
        "            # Определяем ExtraTreeClassifier с переопределённым _find_best_split\n",
        "            class ExtraTreeClassifier(DecisionTreeClassifier):\n",
        "                def _find_best_split(self, node):\n",
        "                    return find_random_split(node, self.X, self.y, is_classifier=True)\n",
        "\n",
        "            tree = ExtraTreeClassifier(max_depth=self.max_depth, min_samples_split=2)\n",
        "            tree.fit(X_sub, y)\n",
        "            self.trees.append((tree, feat_idx))\n",
        "            gc.collect()  # Очистка памяти\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        probs = []\n",
        "        for tree, feat_idx in self.trees:\n",
        "            probs.append(tree.predict_proba(X[:, feat_idx]))\n",
        "        return np.mean(probs, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHz7FTIXKbgX"
      },
      "source": [
        "Обучение и оценка."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0c2q4qBKY5l",
        "outputId": "65db17b5-1dcb-4500-af1a-22fe8df35cae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAM usage before ExtraTrees: 37.2%\n",
            "Пример вероятностей ExtraTrees (первые 5): [[0.8911778  0.1088222 ]\n",
            " [0.89637086 0.10362914]\n",
            " [0.88825904 0.11174096]\n",
            " [0.88671634 0.11328366]\n",
            " [0.88285412 0.11714588]]\n",
            "Уникальные вероятности класса 1: [0.09755338 0.09833466 0.09847226 ... 0.15376495 0.15609039 0.1566234 ]\n",
            "Джини ExtraTrees на валидации: 0.2467177835924379\n",
            "RAM usage after ExtraTrees: 37.2%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(f\"RAM usage before ExtraTrees: {psutil.virtual_memory().percent}%\")\n",
        "\n",
        "# Обучение и оценка\n",
        "et = ExtraTreesClassifier(n_trees=20, max_depth=5, max_features=10, random_seed=42)\n",
        "et.fit(X_train_enc.values, y_train.values)\n",
        "et_proba = et.predict_proba(X_valid_enc.values)\n",
        "et_gini = gini_score(y_valid, et_proba)\n",
        "\n",
        "print(\"Пример вероятностей ExtraTrees (первые 5):\", et_proba[:5])\n",
        "print(\"Уникальные вероятности класса 1:\", np.unique(et_proba[:, 1]))\n",
        "print(f\"Джини ExtraTrees на валидации: {et_gini}\")\n",
        "print(f\"RAM usage after ExtraTrees: {psutil.virtual_memory().percent}%\")\n",
        "\n",
        "# Очистка памяти\n",
        "gc.collect()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
